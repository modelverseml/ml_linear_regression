# Linear Regression : The Foundation of Machine Learning

Linear regression is often considered the starting point for anyone beginning their machine learning journey. At first glance, it seems simple â€” and in many ways, it is. However, there are several constraints and assumptions behind the model that people often overlook when building it. Understanding these boundaries is essential to ensure the modelâ€™s reliability.

At its core, linear regression is about fitting a straight line to data. If the dataset contains only a single feature, the technique is called Simple Linear Regression. When multiple features are involved, it becomes Multiple Linear Regression.


### Assumptions of Linear Regression

For linear regression to give reliable results, certain assumptions must hold true:

- Linearity
  - There should be a linear relationship between the independent variable(s) and the dependent variable.
- Independence of Errors
  - The residuals (errors) should be independent of each other. In other words, one error should not influence another.
- Normality of Errors
  - The residuals should be normally distributed. This helps in making valid inferences about the coefficients.
- Homoscedasticity
  - The residuals should have constant variance across all levels of the independent variables. If variance changes (heteroscedasticity), predictions may become unreliable.
- No Multicollinearity
  - Independent variables should not be highly correlated with each other. Strong correlation among predictors makes it difficult to estimate the effect of each variable accurately.
- No Autocorrelation
  - Errors should not be correlated with themselves over time. Autocorrelation is common in time-series data and violates regression assumptions.
- Feature Relevance (practical consideration)
  - The chosen features should have meaningful predictive power. Irrelevant or noisy features can reduce model performance.

## Linearity 

### Checking the Relationship Between Features and Target

- **Simple Linear Regression (one feature):**  
  - Use a **scatter plot of X vs. Y**.  
  - If the pattern looks roughly linear, overlay a **best-fit line**.
- **Multiple Linear Regression (many features):**  
  A single scatter plot is not enough. Instead, use:
  -  **Pair plots / scatter-matrix** â†’ quick glance at pairwise relations  
  -  **Correlation heatmap** â†’ strength & direction of linear association
 
<p align="center">
<img src="images/linear-nonlinear-corrrelation.jpg" alt="relation" width="50%"/>
</p>

###  Fitting the Best Line

For two features x<sub>1</sub>, x<sub>2</sub> and output y , the model will be

Å· = Î²<sub>0</sub> +  Î²<sub>1</sub> . x<sub>2</sub> +  Î²<sub>2</sub>.x<sub>2</sub> 

We chose  Î²<sub>0</sub>,Î²<sub>1</sub>, Î²<sub>2</sub such that to reduce sum of squared error.

SSE(Î²) = âˆ‘<sub>i=1</sub><sup>n</sup> ( Å·<sub>i</sub> - y<sub>i</sub> )<sup>2</sup> = âˆ‘<sub>i=1</sub><sup>n</sup> ( Î²<sub>0</sub> +  Î²<sub>1</sub> . x<sub>1i</sub> +  Î²<sub>2</sub>.x<sub>2i</sub>  - y<sub>i</sub> )<sup>2</sup>

We solve for B using vectors

X = [1  x<sub>1</sub>  x<sub>2</sub> \]  Y = y

Å· = Î²X and  Î²Ì‚ â€‹= arg min <sub>Î²</sub> â€‹ || XÎ² âˆ’ y || <sup>2</sup>

- closed solution
  - Î²Ì‚ â€‹=(X<sup>T</sup>X)<sup>âˆ’1</sup>X<sup>T</sup>yâ€‹
- Iterative process : Gradient descent
  - Î²â±¼ := Î²â±¼ - Î± * âˆ‚/âˆ‚Î²â±¼ [ SSE(Î²) \]
  - The derivative of SSE with respect to Î²â±¼ is: âˆ‚/âˆ‚Î²â±¼ [ SSE(Î²) \] = -2 âˆ‘ ( yáµ¢ - Å·áµ¢ ) Xáµ¢â±¼
  - Substituting this into the update rule gives: Î²â±¼ = Î²â±¼ + 2Î± âˆ‘ ( yáµ¢ - Å·áµ¢ ) Xáµ¢â±¼
  - We apply this update to all Î² coefficients in each iteration. By adjusting the learning rate Î± and repeating the process iteratively, we gradually minimize the error until the model converges.

**The code for these two methods was provided above as separate functions, please check it out.**

Those implementations were just to demonstrate how the actual calculations work. In practice, you donâ€™t need to perform these steps manuallyâ€”libraries like statsmodels and scikit-learn handle them automatically once you provide the input and output data.

## Error Independence:

What does error independece mean actuall ? 

Error independence means that the error (or residual) for one observation should not be correlated with the error of another observation.

For example, suppose we have two instances in the dataset:

- Instance 1 â†’ (x1áµ¢, x2áµ¢, yáµ¢)

- Instance 2 â†’ (x1â±¼, x2â±¼, yâ±¼)

The error for the first instance is:
eáµ¢ = Å·áµ¢ - yáµ¢

And the error for the second instance is:
eâ±¼ = Å·â±¼ - yâ±¼

For independence, eáµ¢ should not provide any information about eâ±¼. In other words, the error of one data point should not influence or predict the error of another.

To check this assumption, we usually:

- Plot the residuals against the order of observations (or time, if itâ€™s time-series data).
- If the errors are independent â†’ the plot should look like random scatter around zero with no visible pattern.
- If not â†’ you may see trends, cycles, or clustering, which indicates correlation among errors.

<p align="center">
<img src="images/errors_independece.webp" alt="independence" width="50%"/>
</p>

## Normality of Error Terms:

It indicates that the errors (residuals) from predictions are random and follow a normal distribution.

This assumption matters because:

- It ensures that the statistical inference we do (like hypothesis testing and confidence intervals) is valid.

- If errors are normally distributed, the estimated coefficients (Î²) are unbiased and efficient.

We usually check this by:

- Histogram of residuals â†’ should look bell-shaped.

<p align="center">
<img src="images/errors_normality.webp" alt="Normality" width="50%"/>
</p>

## Homoscedasticity:

It means that the errors (residuals) should have constant variance across all levels of the predicted values (Å·) or input features.

- If homoscedasticity holds â†’ the spread of residuals remains roughly the same for all fitted values.

- If violated (called heteroscedasticity) â†’ the variance of errors changes, often seen as a â€œfunnelâ€ or â€œconeâ€ shape in residual plots.

Why it matters:

- Homoscedasticity ensures that the modelâ€™s estimates of standard errors are correct.

- If violated, hypothesis tests (t-tests, F-tests) and confidence intervals may become unreliable.

How to check:

- Residuals vs fitted values plot â†’ should show random scatter with no clear pattern and roughly equal spread

<p align="center">
<img src="images/constant_variation.webp" alt="variation" width="50%"/>
</p>

## Multicollinearity

Multicollinearity occurs when two or more input features are highly correlated (dependent) with each other. Because of this, the model struggles to distinguish the individual effect of each feature on the output.

Suppose we have three features: f1, f2, f3 and an output y, where

f3 = f1 + f2

In this case, f3 is completely dependent on f1 and f2. Because of this strong linear relationship, the regression model cannot clearly separate the individual effects of f1, f2, and f3 on y.

### How to check:
- Checking the correlation coffient matrix or heatmap
- Variance inflation factors > 10 usually indicates problematic multicollinearity.

### Variance Inflation Factor (VIF) : 

VIF is a metric used to detect multicollinearity among input features in regression. It measures how much the variance of a regression coefficient (Î²) is inflated because of linear dependence with other features.

Formula:

For a feature X<sub>j</sub> 

VIF<sub>j</sub>  = 1 / (1âˆ’R<sub>j</sub><sup>2</sup>)
â€‹

where  
R<sub>j</sub><sup>2</sup> - is the coefficient of determination obtained by regressing X<sub>j</sub>  on all the other features. (Explain this later)

Interpretation:

- VIF = 1 â†’ No correlation with other features.

- VIF between 1 and 5 â†’ Moderate correlation (usually acceptable).

- VIF > 10 â†’ High multicollinearity (problematic, needs attention).

## Feature Relevance

We canâ€™t simply build a model with every feature available. For example, suppose we have 100 features, all independent of each other. Do we really need to use all 100? Not necessarily.

Instead, we should focus on the most important featuresâ€”the ones that contribute most to explaining the variation in the target variable. Often, selecting the top 15â€“20 features is enough to build a strong model.

But how do we determine which features are most important?

### Approaches to Identify Feature Relevance:

- Model-based selection

  - Build the model with all features, then reduce features step by step and check performance.

  - Example: Recursive Feature Elimination (RFE).
 
- Incremental feature addition

  - Start with a smaller subset of features (say 10).

  - Add more features gradually and evaluate performance.

  - Stop when the model is already able to explain ~90% of the variation in the data.
 
- Feature importance scores

  - Use techniques like coefficients in linear regression, feature importance in tree-based models, or LASSO regularization to rank features.

## Hypothesis Testing

In linear regression, hypothesis testing is used to check whether the input features (independent variables) have a statistically significant effect on the output (dependent variable).

We mainly test hypotheses about the regression coefficients (Î² values).

First we start with hypothesis as follows
- Null Hypothesis (H<sub>0</sub>): Î²<sub>j</sub> = 0 â†’ The feature ğ‘‹<sub>ğ‘—</sub> has no effect on the output
- Alternative Hypothesis (Hâ‚): Î²<sub>j</sub> â‰  0 â†’ The feature ğ‘‹<sub>ğ‘—</sub> has a significant effect on the output.

### T - Distribution

t-statistic = ( Î²Ì‚ - Î¼ )/ SE(Î²Ì‚<sub>j</sub>)â€‹â€‹ = ( Î²Ì‚ - 0 )/ SE(Î²Ì‚<sub>j</sub>)â€‹â€‹

-  If t <sub>statistic</sub> > t <sub>critical</sub> (Î±=0.05 ), reject H <sub>0</sub> â†’ Î²<sub>j</sub> is important

### P - value

- If p-value < significance level (e.g., 0.05), reject Hâ‚€ â†’ the feature is important.

### Global Test (F-test):

- Hypothesis:
  - Null Hypothesis (H<sub>0</sub>): ALL Î²<sub>j</sub> = 0 
  - Alternative Hypothesis (Hâ‚): At least one Î²<sub>j</sub> â‰  0
 
  F <sub>statistic</sub> > F <sub>critical</sub> , reject H<sub>0</sub> â†’ At least one feature is important

**For a regression model, we prefer:**
- Low p-values for individual coefficients (Î²<sub>j</sub>)
- Large |t-statistics| for coefficients
- High F-statistic overall

## Overfitting and Underfitting in Linear Regression

So far, we have focused on the assumptions and requirements before training a regression model and how to interpret the results. But we havenâ€™t yet discussed an equally important aspect: how well the trained model is actually performing.

After training, there are two possible scenarios:

The model performs well on the training set but poorly on the test set â†’ this is called overfitting.

The model performs poorly on both the training set and the test set â†’ this is called underfitting.

These behaviors are closely related to the concepts of bias and variance:

- Bias : It measures how far a modelâ€™s predictions are from the actual values, especially on unseen (future) data.
    - Models with high bias are too simple, missing important patterns in the data (underfitting).
    - Linear algorithms usually have higher biasâ€”they are fast and easy to understand but less flexible, which limits their predictive power on complex problems.

- Variance : how sensitive is the model for the change of the traning data
    - Models with high variance fit the training data too closely, even capturing noise. This makes them unstableâ€”small changes in the dataset lead to large changes in predictions (overfitting).
    - A good model should have low variance, meaning it generalizes well and captures the true underlying patterns without being overly sensitive to fluctuations in the training data.

<p align="center">
<img src="images/trade_off.png" alt="independence" height="300" width="45%"/>
<img src="images/bullseye.png" alt="independence" height="300" width="45%"/>
</p>

For an ideal model, both bias and variance should be low â€” but in reality, such a model doesnâ€™t exist. Instead, we face a trade-off between the two.

- Underfitting: Bias is high and variance is low (model is too simple, missing patterns).

- Overfitting: Bias is low but variance is high (model is too complex, capturing noise).

Having too many features can increase model complexity and lead to overfitting. Thatâ€™s why feature selection or dimensionality reduction is often applied to simplify the model and improve generalization.

## Metrics

So far, weâ€™ve covered all the prerequisites and steps for model tuning. Now, the next step is to evaluate our regression model. To do this, we use a few important metrics, which help us measure how well the model performs on unseen data. The most commonly used metrics are:

<p align="center">
<img src="images/metrics.webp" alt="metrics"  width="50%"/>
</p>

### Mean Absolute Error (MAE)

The Mean Absolute Error is the average of the absolute differences between the predicted and actual values.

Formula : 

MAE : (1 / n ) Î£ | y<sub>i</sub> â€“ Å·<sub>i</sub> |

- Easy to interpret since it measures the average error directly.
- However, it is less sensitive to outliers, meaning extreme values do not heavily impact the error.

### Mean Squared Error (MSE)

The Mean Squared Error is the average of the squared differences between the predicted and actual values.

Formula : 

MSE : (1 / n ) Î£ ( y<sub>i</sub> â€“ Å·<sub>i</sub> )<sup>2</sup>
  
- Squaring the errors gives more weight to larger errors.

- This makes MSE more sensitive to outliers compared to MAE.

### Root Mean Squared Error (RMSE)

The Root Mean Squared Error is the square root of the average of the squared differences between the predicted and actual values.

Formula : 

RMSE : âˆš ((1 / n ) Î£ ( y<sub>i</sub> â€“ Å·<sub>i</sub> )<sup>2</sup>)

- RMSE penalizes large errors more strongly than MAE.

- Unlike MSE, RMSE is expressed in the same units as the output variable, making it easier to interpret in practice.

### RÂ² Score (Coefficient of Determination)

The RÂ² score measures how well the regression model explains the variability of the target variable compared to its mean.

Formula 

RÂ² = 1 â€“ (SSE / SST)

SSE = Î£ (yáµ¢ â€“ Å·áµ¢)Â² â†’ sum of squared errors (residuals)

SST = Î£ (yáµ¢ â€“ È³)Â² â†’ total variation in the data

- RÂ² ranges between 0 and 1.
- A value close to 1 means the model explains most of the variance in the data, while a value near 0 means poor explanatory power.


### Adjusted RÂ² 

The Adjusted RÂ² modifies the RÂ² score to account for the number of predictors (features) in the model.


Formula:
Adjusted RÂ² = 1 â€“ [ (1 â€“ RÂ²) Ã— (n â€“ 1) / (n â€“ p â€“ 1) \]

- n = number of observations

- p = number of predictors (features)

- Unlike RÂ², which can artificially increase when more features are added, Adjusted RÂ² penalizes unnecessary predictors. This is because adding too many features can increase model complexity and lead to overfitting

- It is more reliable in multiple linear regression, ensuring that only meaningful features improve the score.

